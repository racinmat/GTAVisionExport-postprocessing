{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import visualization\n",
    "import os\n",
    "from gta_math import points_to_homo, ndc_to_view, construct_proj_matrix, view_to_world, construct_view_matrix\n",
    "from visualization import load_depth, load_stencil, save_pointcloud_csv, bbox_from_string, are_buffers_same_as_previous, \\\n",
    "    is_first_record_in_run, camera_to_string\n",
    "import progressbar\n",
    "from joblib import Parallel, delayed\n",
    "from configparser import ConfigParser\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from functools import lru_cache\n",
    "import scipy.io as sio\n",
    "import tifffile\n",
    "from toyota import json_to_toyota_format, json_to_toyota_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_file = \"gta-postprocessing.ini\"\n",
    "visualization.multi_page = False\n",
    "visualization.ini_file = ini_file\n",
    "visualization.use_cache = False\n",
    "\n",
    "conn = visualization.get_connection_pooled()\n",
    "cur = conn.cursor()\n",
    "\n",
    "CONFIG = ConfigParser()\n",
    "CONFIG.read(ini_file)\n",
    "in_directory = CONFIG[\"Images\"][\"Tiff\"]\n",
    "out_directory = r'D:\\output-datasets\\onroad-3-toyota'\n",
    "\n",
    "# for some datasets we don't need entities, which produces lot smaller json files\n",
    "# for some datasets, we want each camera data to be in separate folder\n",
    "# sometimes buffers are not synced and don't correspond to current camera. \n",
    "# they can be recognized by having same data as previous record\n",
    "discard_invalid_buffers = True\n",
    "\n",
    "# default settings\n",
    "include_entities = True\n",
    "directory_per_camera = False\n",
    "depth_in_tiff = False\n",
    "scene_index_naming = False\n",
    "export_depth = True\n",
    "export_stencil = True\n",
    "toyota_data_format = False\n",
    "\n",
    "# modes\n",
    "mode = 'toyota'\n",
    "\n",
    "\n",
    "modes = ['onroad', 'offroad', 'toyota']\n",
    "if mode not in modes:\n",
    "    raise Exception('Invalid mode selected')\n",
    "\n",
    "if mode == 'onroad':\n",
    "    include_entities = True\n",
    "    directory_per_camera = False\n",
    "    depth_in_tiff = False\n",
    "    scene_index_naming = False\n",
    "    export_depth = True\n",
    "    export_stencil = True\n",
    "    toyota_data_format = False\n",
    "elif mode == 'offroad':\n",
    "    include_entities = False\n",
    "    directory_per_camera = True\n",
    "    depth_in_tiff = True # use .tiff format to store depth directly in NDC\n",
    "    scene_index_naming = True\n",
    "    export_depth = True\n",
    "    export_stencil = True\n",
    "    toyota_data_format = False\n",
    "elif mode == 'toyota':\n",
    "    include_entities = True\n",
    "    directory_per_camera = False\n",
    "    depth_in_tiff = False\n",
    "    scene_index_naming = False\n",
    "    export_depth = False\n",
    "    export_stencil = False\n",
    "    toyota_data_format = True\n",
    "    \n",
    "\n",
    "if scene_index_naming and not directory_per_camera:\n",
    "    print('BEWARE, THE CONFIGURATION IS WRONG AND YOU MIGHT MISS SOME FILES, MULTIPLE FILES WITH SAME NAME IN ONE DIRECTORY WILL BE THERE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31027 snapshots\n",
      "{'camera_-0.80_0.80_0.40__0.00_0.00_90.00': '0', 'camera_0.00_-2.30_0.30__0.00_0.00_180.00': '1', 'camera_0.00_2.00_0.30__0.00_0.00_0.00': '2', 'camera_0.80_0.80_0.40__0.00_0.00_270.00': '3'}\n"
     ]
    }
   ],
   "source": [
    "#run_id = 19\n",
    "#run_id = 3677\n",
    "run_id = 3961\n",
    "\n",
    "cur.execute(\"\"\"SELECT snapshot_id, imagepath, cam_near_clip, camera_fov, width, height, timestamp, timeofday, width, height, \\\n",
    "      ARRAY[st_x(camera_relative_rotation), st_y(camera_relative_rotation), st_z(camera_relative_rotation)] as camera_relative_rotation, \\\n",
    "      ARRAY[st_x(camera_relative_position), st_y(camera_relative_position), st_z(camera_relative_position)] as camera_relative_position, \\\n",
    "      ARRAY[st_x(camera_pos), st_y(camera_pos), st_z(camera_pos)] as camera_pos, \\\n",
    "      ARRAY[st_x(camera_rot), st_y(camera_rot), st_z(camera_rot)] as camera_rot, \\\n",
    "      ARRAY[st_x(current_target), st_y(current_target), st_z(current_target)] as current_target, \\\n",
    "      currentweather, scene_id, run_id \\\n",
    "      FROM snapshots \\\n",
    "      WHERE run_id = {}\n",
    "      ORDER BY timestamp ASC \\\n",
    "    \"\"\".format(run_id))\n",
    "\n",
    "results = []\n",
    "for row in cur:\n",
    "    res = dict(row)\n",
    "    if res['camera_fov'] == 0 and res['cam_near_clip'] == 0:\n",
    "        continue  # somehow malformed data, skipping them\n",
    "    res['camera_rot'] = np.array(res['camera_rot'])\n",
    "    res['camera_pos'] = np.array(res['camera_pos'])\n",
    "    res['camera_relative_rotation'] = np.array(res['camera_relative_rotation'])\n",
    "    res['camera_relative_position'] = np.array(res['camera_relative_position'])\n",
    "    res['current_target'] = np.array(res['current_target'])\n",
    "    res['view_matrix'] = construct_view_matrix(res['camera_pos'], res['camera_rot'])\n",
    "    res['proj_matrix'] = construct_proj_matrix(res['height'], res['width'], res['camera_fov'], res['cam_near_clip'])\n",
    "    results.append(res)\n",
    "\n",
    "print('There are {} snapshots'.format(len(results)))\n",
    "\n",
    "# because sometimes I use two cameras heading same direction, pair (position, rotation) is unique identifier\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT DISTINCT \\\n",
    "      ARRAY[st_x(camera_relative_rotation), st_y(camera_relative_rotation), st_z(camera_relative_rotation)] as camera_relative_rotation, \n",
    "      ARRAY[st_x(camera_relative_position), st_y(camera_relative_position), st_z(camera_relative_position)] as camera_relative_position \n",
    "      FROM snapshots \\\n",
    "      WHERE run_id = {} AND camera_fov != 0 \\\n",
    "      ORDER BY camera_relative_position, camera_relative_rotation ASC \\\n",
    "    \"\"\".format(run_id))\n",
    "\n",
    "camera_names = {}\n",
    "for i, row in enumerate(cur):\n",
    "    camera_name = camera_to_string(row)\n",
    "    camera_names[camera_name] = str(i)\n",
    "    \n",
    "print(camera_names)\n",
    "\n",
    "# this is for file naming by scene index\n",
    "if scene_index_naming:\n",
    "    # then I get sorted all scene ids\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SELECT scene_id, min(timestamp) \n",
    "            FROM snapshots\n",
    "            WHERE run_id = %(run_id)s\n",
    "            GROUP BY scene_id\n",
    "          ORDER BY min(timestamp) ASC\n",
    "        \"\"\", {'run_id': run_id})\n",
    "\n",
    "    scenes = {}\n",
    "    for i, row in enumerate(cur):\n",
    "        scenes[row['scene_id']] = f'{i:06}'  # prepend zeros to 5 places\n",
    "\n",
    "    # then I take all imagepaths for all scene ids\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"SELECT scene_id, imagepath\n",
    "          FROM snapshots \\\n",
    "          WHERE run_id = %(run_id)s\n",
    "          ORDER BY timestamp ASC\n",
    "        \"\"\", {'run_id': run_id})\n",
    "\n",
    "    file_names = {}\n",
    "    for i, row in enumerate(cur):\n",
    "        file_names[row['imagepath']] = row['scene_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_name(name):\n",
    "    return os.path.basename(os.path.splitext(name)[0])\n",
    "\n",
    "def get_main_image_name(cameras):\n",
    "    for cam in cameras:\n",
    "        # this is the main camera\n",
    "        if np.array_equal(cam['camera_relative_rotation'], [0, 0, 0]):\n",
    "            return cam['imagepath']\n",
    "    raise Exception('no main image')\n",
    "\n",
    "def load_entities_data(snapshot_id):\n",
    "    conn = visualization.get_connection_pooled()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # start = time.time()\n",
    "\n",
    "    cur.execute(\"\"\"SELECT bbox, \\\n",
    "        ARRAY[st_x(pos), st_y(pos), st_z(pos)] as pos, \\\n",
    "        ARRAY[st_x(rot), st_y(rot), st_z(rot)] as rot, \\\n",
    "        ARRAY[st_xmin(bbox3d), st_xmax(bbox3d), st_ymin(bbox3d), st_ymax(bbox3d), st_zmin(bbox3d), st_zmax(bbox3d)] as bbox3d, \\\n",
    "         type, class, handle, snapshot_id \\\n",
    "        FROM detections \\\n",
    "        WHERE snapshot_id = '{}' \\\n",
    "        \"\"\".format(snapshot_id))\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('time to load from db', end - start)\n",
    "    # start = time.time()\n",
    "\n",
    "    # print(size)\n",
    "    results = []\n",
    "    for row in cur:\n",
    "        res = dict(row)\n",
    "        res['model_sizes'] = np.array(res['bbox3d'])\n",
    "        res['bbox'] = bbox_from_string(res['bbox'])\n",
    "        res['pos'] = np.array(res['pos'])\n",
    "        res['rot'] = np.array(res['rot'])\n",
    "        results.append(res)\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('time to convert arrays to numpy', end - start)\n",
    "    # start = time.time()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_rgb(in_directory, out_directory, out_name, name, out_format):\n",
    "    outfile = os.path.join(out_directory, \"{}.{}\".format(out_name, out_format))\n",
    "    # print(outfile)\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        infile = os.path.join(in_directory, name)\n",
    "        im = Image.open(infile)\n",
    "        im = im.convert(mode=\"RGB\")\n",
    "        im.save(outfile)\n",
    "    except OSError:\n",
    "        # print(\"Skipping invalid file {}\".format(name))\n",
    "        return\n",
    "\n",
    "\n",
    "def convert_depth(in_directory, out_directory, out_name, name, out_format):\n",
    "    outfile = os.path.join(out_directory, \"{}.{}\".format(out_name, out_format))\n",
    "    # print(outfile)\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        infile = os.path.join(in_directory, name)\n",
    "        depth = load_depth(name)\n",
    "        if out_format in ['png', 'jpg']:\n",
    "            # print('depth min before: ', np.min(depth))\n",
    "            # print('depth max before: ', np.max(depth))\n",
    "            depth = depth * np.iinfo(np.uint16).max\n",
    "            # print('depth min after: ', np.min(depth))\n",
    "            # print('depth max after: ', np.max(depth))\n",
    "            im = Image.fromarray(depth.astype(np.int32), mode=\"I\")\n",
    "            im.save(outfile)\n",
    "        elif out_format == 'mat':\n",
    "            sio.savemat(outfile, {'depth': depth}, do_compression=True)\n",
    "        elif out_format == 'tiff':\n",
    "            tifffile.imsave(outfile, depth, compress='lzma')\n",
    "    except OSError:\n",
    "        # print(\"Skipping invalid file {}\".format(name))\n",
    "        return\n",
    "\n",
    "def convert_stencil(in_directory, out_directory, out_name, name, out_format):\n",
    "    outfile = os.path.join(out_directory, \"{}.{}\".format(out_name, out_format))\n",
    "    # print(outfile)\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        infile = os.path.join(in_directory, name)\n",
    "        stencil = load_stencil(name)\n",
    "        im = Image.fromarray(stencil.astype(np.uint8), mode=\"L\")\n",
    "        im.save(outfile)\n",
    "    except OSError:\n",
    "        # print(\"Skipping invalid file {}\".format(name))\n",
    "        return\n",
    "\n",
    "def convert_toyota_annotations(in_directory, out_directory, out_name, name, json_data):\n",
    "    outfile = os.path.join(out_directory, '{}.txt'.format(out_name))\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "    \n",
    "    stencil = load_stencil(name)\n",
    "    depth = load_depth(name)\n",
    "\n",
    "    txt_data = json_to_toyota_format(json_data, depth, stencil)\n",
    "    with open(outfile, mode='w+') as f:\n",
    "        f.writelines(txt_data)\n",
    "\n",
    "\n",
    "def convert_toyota_cameras(in_directory, out_directory, out_name, json_data):\n",
    "    outfile = os.path.join(out_directory, '{}.cam'.format(out_name))\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "        \n",
    "    cam_data = json_to_toyota_calibration(json_data)\n",
    "    with open(outfile, mode='w+') as f:\n",
    "        f.writelines(cam_data)\n",
    "\n",
    "def get_out_directory(out_directory, res, directory_per_camera):\n",
    "    if directory_per_camera:\n",
    "        return os.path.join(out_directory, camera_names[camera_to_string(res)])\n",
    "    else:\n",
    "        return out_directory\n",
    "\n",
    "def try_dump_snapshot_to_dataset(in_directory, out_directory, res, run_id):\n",
    "    try:\n",
    "        dump_snapshot_to_dataset(in_directory, out_directory, res, run_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "def dump_snapshot_to_dataset(in_directory, out_directory, res, run_id):\n",
    "    if 'pbar' in globals() and 'counter' in globals():\n",
    "        global counter\n",
    "        counter += 1\n",
    "        pbar.update(counter)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    if not os.path.exists(out_directory):\n",
    "        os.makedirs(out_directory)\n",
    "\n",
    "    if discard_invalid_buffers and (not is_first_record_in_run(res, run_id)) and \\\n",
    "        are_buffers_same_as_previous(res):\n",
    "        print('skipping record wih invalid buffers in snapshot {}, filename {} and camera {}'\n",
    "              .format(res['snapshot_id'], res['imagepath'], res['camera_relative_position'].tolist()))\n",
    "        return\n",
    "    \n",
    "    name = res['imagepath']\n",
    "    out_name = name\n",
    "    if scene_index_naming:\n",
    "        out_name = scenes[file_names[name]]\n",
    "\n",
    "    convert_rgb(in_directory, out_directory, out_name, name+'.tiff', 'jpg')\n",
    "    if export_depth:\n",
    "        if depth_in_tiff:\n",
    "            convert_depth(in_directory, out_directory, out_name+'-depth', name, 'tiff')\n",
    "        else:\n",
    "            convert_depth(in_directory, out_directory, out_name+'-depth', name, 'png')\n",
    "    \n",
    "    if export_stencil:\n",
    "        convert_stencil(in_directory, out_directory, out_name+'-stencil', name, 'png')\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('time to generate images', end - start)\n",
    "    # start = time.time()\n",
    "    \n",
    "    outfile = os.path.join(out_directory, '{}.json'.format(out_name))\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "\n",
    "    if include_entities:\n",
    "        data = load_entities_data(res['snapshot_id'])\n",
    "\n",
    "        # end = time.time()\n",
    "        # print('loading entities data', end - start)\n",
    "        # start = time.time()\n",
    "    \n",
    "        json_entities_data = []\n",
    "        for i in data:\n",
    "            json_entity = {\n",
    "                'model_sizes': i['model_sizes'].tolist(),\n",
    "                'bbox': i['bbox'].tolist(),\n",
    "                'pos': i['pos'].tolist(),\n",
    "                'rot': i['rot'].tolist(),\n",
    "                'class': i['class'],\n",
    "                'handle': i['handle'],\n",
    "                'type': i['type'],\n",
    "            }\n",
    "            json_entities_data.append(json_entity)\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('dumping entities to json', end - start)\n",
    "    # start = time.time()\n",
    "\n",
    "    json_data = {\n",
    "        'imagepath': res['imagepath'],\n",
    "        'timestamp': res['timestamp'].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'timeofday': res['timeofday'].strftime(\"%H:%M:%S\"),\n",
    "        'currentweather': res['currentweather'],\n",
    "        'width': res['width'],\n",
    "        'height': res['height'],\n",
    "        'snapshot_id': res['snapshot_id'],\n",
    "        'scene_id': res['scene_id'],\n",
    "        'run_id': res['run_id'],\n",
    "        'camera_rot': res['camera_rot'].tolist(),\n",
    "        'camera_pos': res['camera_pos'].tolist(),\n",
    "        'camera_fov': res['camera_fov'],\n",
    "        'camera_relative_rotation': res['camera_relative_rotation'].tolist(),\n",
    "        'camera_relative_position': res['camera_relative_position'].tolist(),\n",
    "        'current_target': res['current_target'].tolist(),\n",
    "        'view_matrix': res['view_matrix'].tolist(),\n",
    "        'proj_matrix': res['proj_matrix'].tolist(),\n",
    "    }\n",
    "    \n",
    "    if include_entities:\n",
    "        json_data['entities'] = json_entities_data\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('dumping base object to json', end - start)\n",
    "    # start = time.time()\n",
    "\n",
    "    if toyota_data_format:\n",
    "        convert_toyota_annotations(in_directory, out_directory, name, name, json_data)\n",
    "        convert_toyota_cameras(in_directory, out_directory, name, json_data)\n",
    "    else:\n",
    "        with open(outfile, 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "    \n",
    "    # end = time.time()\n",
    "    # print('persisting json to file', end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dumping all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workers = 6\n",
    "\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Counter(), ' ', progressbar.Bar(), ' ',\n",
    "           progressbar.FileTransferSpeed()]\n",
    "\n",
    "pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(results)).start()\n",
    "counter = 0\n",
    "\n",
    "Parallel(n_jobs=workers, backend='threading')(delayed(dump_snapshot_to_dataset)(in_directory, get_out_directory(out_directory, i, directory_per_camera), i, run_id) for i in results)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copying to linux into datagrid dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% 812 |#####                                                     |   2.0 B/s"
     ]
    }
   ],
   "source": [
    "def copy_file(in_path, out_path):\n",
    "    if os.path.exists(out_path):\n",
    "        return\n",
    "    copyfile(in_path, out_path)\n",
    "\n",
    "def copy_files(in_dir, out_dir, name):\n",
    "    if 'pbar' in globals() and 'counter' in globals():\n",
    "        global counter\n",
    "        counter += 1\n",
    "        pbar.update(counter)\n",
    "\n",
    "    rgb_name = name+'.jpg'\n",
    "    json_name = name+'.json'\n",
    "    depth_name = name+'-depth.png'\n",
    "    stencil_name = name+'-stencil.png'\n",
    "    \n",
    "    copyfile(os.path.join(in_dir, rgb_name), os.path.join(out_dir, rgb_name))\n",
    "    copyfile(os.path.join(in_dir, json_name), os.path.join(out_dir, json_name))\n",
    "    copyfile(os.path.join(in_dir, depth_name), os.path.join(out_dir, depth_name))\n",
    "    copyfile(os.path.join(in_dir, stencil_name), os.path.join(out_dir, stencil_name))\n",
    "\n",
    "workers = 8\n",
    "\n",
    "windows_in_directory = r'D:\\output-datasets\\offroad-6'\n",
    "linux_out_directory = r'Z:\\offroad-6'\n",
    "\n",
    "if not os.path.exists(linux_out_directory):\n",
    "    os.makedirs(linux_out_directory)\n",
    "\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Counter(), ' ', progressbar.Bar(), ' ',\n",
    "           progressbar.FileTransferSpeed()]\n",
    "\n",
    "pbar = progressbar.ProgressBar(widgets=widgets, max_value=len(results)).start()\n",
    "counter = 0\n",
    "\n",
    "Parallel(n_jobs=workers, backend='threading')(\n",
    "    delayed(copy_files)(windows_in_directory, linux_out_directory, i['imagepath']) for i in results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copying original tiffs to linux into datagrid dir, only one run from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 42 |                                                           |   0.7 B/s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-505770c9802e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m Parallel(n_jobs=workers, backend='threading')(\n\u001b[1;32m---> 31\u001b[1;33m     delayed(copy_files)(in_directory, out_directory, i['imagepath']) for i in results)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def copy_file(in_path, out_path):\n",
    "    if os.path.exists(out_path):\n",
    "        return\n",
    "    copyfile(in_path, out_path)\n",
    "\n",
    "def copy_files(in_dir, out_dir, name):\n",
    "    if 'pbar' in globals() and 'counter' in globals():\n",
    "        global counter\n",
    "        counter += 1\n",
    "        pbar.update(counter)\n",
    "\n",
    "    rgb_name = name+'.tiff'\n",
    "    depth_name = name+'-depth.tiff'\n",
    "    stencil_name = name+'-stencil.tiff'\n",
    "    \n",
    "    copyfile(os.path.join(in_dir, rgb_name), os.path.join(out_dir, rgb_name))\n",
    "    copyfile(os.path.join(in_dir, depth_name), os.path.join(out_dir, depth_name))\n",
    "    copyfile(os.path.join(in_dir, stencil_name), os.path.join(out_dir, stencil_name))\n",
    "\n",
    "workers = 8\n",
    "\n",
    "out_directory = r'Z:\\offroad-4-orig'\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Counter(), ' ', progressbar.Bar(), ' ',\n",
    "           progressbar.FileTransferSpeed()]\n",
    "\n",
    "pbar = progressbar.ProgressBar(widgets=widgets, max_value=len(results)).start()\n",
    "counter = 0\n",
    "\n",
    "Parallel(n_jobs=workers, backend='threading')(\n",
    "    delayed(copy_files)(in_directory, out_directory, i['imagepath']) for i in results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking if all converted images are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 15 |                                                           |   3.9 B/sC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tifffile\\tifffile.py:2642: RuntimeWarning: py_decodelzw encountered unexpected end of stream\n",
      "  strip = decompress(strip)\n",
      " 13% 968 |#######                                                   |   3.9 B/sC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tifffile\\tifffile.py:2577: UserWarning: unpack: string size must be a multiple of element size\n",
      "  warnings.warn(\"unpack: %s\" % e)\n",
      "100% 7250 |#########################################################|   4.4 B/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def check_invalid_buffers(in_directory, out_directory, res, run_id, include_entities):\n",
    "    if 'pbar' in globals() and 'counter' in globals():\n",
    "        global counter\n",
    "        counter += 1\n",
    "        pbar.update(counter)\n",
    "    \n",
    "    # skip non-existing files in out_directory, so just check existence of some of 4 files\n",
    "    outfile = os.path.join(out_directory, '{}.json'.format(res['imagepath']))\n",
    "    if not os.path.exists(outfile):\n",
    "        return\n",
    "    \n",
    "    if discard_invalid_buffers and (not is_first_record_in_run(res, run_id)) and \\\n",
    "        are_buffers_same_as_previous(res):\n",
    "        print('found record wih invalid buffers in snapshot {}, filename {} and camera {}'\n",
    "              .format(res['snapshot_id'], res['imagepath'], res['camera_relative_position'].tolist()))\n",
    "        return\n",
    "\n",
    "\n",
    "workers = 8\n",
    "\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Counter(), ' ', progressbar.Bar(), ' ',\n",
    "           progressbar.FileTransferSpeed()]\n",
    "\n",
    "pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(results)).start()\n",
    "counter = 0\n",
    "\n",
    "Parallel(n_jobs=workers, backend='threading')(delayed(check_invalid_buffers)(in_directory, get_out_directory(out_directory, i, directory_per_camera), i, run_id, include_entities) for i in results)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dumping one snapshot, for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_snapshot_to_dataset(in_directory, out_directory, results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = np.array(Image.open(os.path.join(out_directory, results[0]['imagepath']+'-depth.png')))\n",
    "print(np.min(im))\n",
    "print(np.max(im))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[0]['snapshot_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cam_near_clip': 0.15,\n",
       " 'camera_fov': 90.0,\n",
       " 'camera_pos': array([ 2524.83105469,  3334.53613281,    53.38750458]),\n",
       " 'camera_relative_position': array([-0.06      ,  1.5       ,  1.07649994]),\n",
       " 'camera_relative_rotation': array([ 0.,  0.,  0.]),\n",
       " 'camera_rot': array([-1.26793635, -4.02545595, -0.25146627]),\n",
       " 'current_target': array([2586.0, 3490.0, None], dtype=object),\n",
       " 'currentweather': 'Clear',\n",
       " 'height': 1052,\n",
       " 'imagepath': '2018-08-13--11-15-01--499',\n",
       " 'proj_matrix': array([[  5.49634274e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "        [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   1.49945045e-05,\n",
       "           1.50002249e-01],\n",
       "        [  0.00000000e+00,   0.00000000e+00,  -1.00000000e+00,\n",
       "           0.00000000e+00]]),\n",
       " 'snapshot_id': 466517,\n",
       " 'timeofday': datetime.time(12, 5, 35),\n",
       " 'timestamp': datetime.datetime(2018, 8, 13, 11, 15, 1, 499713, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)),\n",
       " 'view_matrix': array([[  9.97523352e-01,  -4.37807276e-03,   7.01996747e-02,\n",
       "          -2.50772688e+03],\n",
       "        [ -7.00846934e-02,   2.24356686e-02,   9.97288713e-01,\n",
       "           4.88967069e+01],\n",
       "        [ -5.94117919e-03,  -9.99738702e-01,   2.20732675e-02,\n",
       "           3.34748686e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           1.00000000e+00]]),\n",
       " 'width': 1914}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = results[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera_-0.06_1.50_1.08__0.00_0.00_0.00': '0',\n",
       " 'camera_-0.06_17.50_25.08__270.00_0.00_90.00': '5',\n",
       " 'camera_-0.06_33.50_6.08__-30.00_0.00_180.00': '2',\n",
       " 'camera_-16.06_17.50_6.08__-30.00_0.00_270.00': '3',\n",
       " 'camera_0.48_1.50_1.08__0.00_0.00_0.00': '4',\n",
       " 'camera_15.94_17.50_6.08__-30.00_0.00_90.00': '1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
